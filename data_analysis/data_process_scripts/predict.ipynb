{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start loading\n",
      "0kernel /home/pzou/projects/Power_Signature/results_backup/mybench/p100/mem_trace-combine/gesummv5_kernel.csv\n",
      "0kernel /home/pzou/projects/Power_Signature/results_backup/mybench/p100/mem_trace-combine/gesummv7_kernel.csv\n",
      "0kernel /home/pzou/projects/Power_Signature/results_backup/mybench/p100/mem_trace-combine/heartwall2_kernel.csv\n",
      "Done loading\n",
      "1/1 [==============================] - 1s 1s/sample - loss: 0.0516 - acc: 1.0000\n",
      "1/1 [==============================] - 1s 1s/sample - loss: 0.0517 - acc: 1.0000\n",
      "Time:  70.02522715553641\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "import math\n",
    "import itertools\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "from decimal import Decimal\n",
    "import glob\n",
    "import csv\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.svm \n",
    "import sklearn.metrics\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import pickle\n",
    "import timeit\n",
    "\n",
    "\n",
    "counts=[0,0,0,0]\n",
    "\n",
    "#%%\n",
    "\n",
    "def get_app_list(fileName):\n",
    "    \"\"\"get metrics list\n",
    "\n",
    "    Arguments:\n",
    "        fileName {string} -- the file name of application file with absolute path\n",
    "\n",
    "    Returns:\n",
    "        app {list} -- the app  list\n",
    "    \"\"\"\n",
    "    apps = []\n",
    "    with open(fileName) as f:\n",
    "        for line in f.readlines():\n",
    "            if not line.startswith('#'):\n",
    "                words = line.strip().split(',')\n",
    "                app = words[0].strip()\n",
    "                app_num = words[1].strip()\n",
    "                apps.append([app, app_num])\n",
    "        f.close()\n",
    "    return apps\n",
    "\n",
    "\n",
    "def load_kernel_file(filepath):\n",
    "    if not os.path.exists(filepath):\n",
    "        print(filepath)\n",
    "        return \n",
    "    dataframe = pd.read_csv(filepath)\n",
    "    if (dataframe.isnull().values.any()):\n",
    "        print(\"kernel\", filepath)\n",
    "    if (dataframe.shape[1] == 5 and dataframe.shape[0] == 0):\n",
    "        print(\"0kernel\", filepath)\n",
    "        counts[0] +=1\n",
    "        dataframe.loc[0] = [-1, -1, -1, -1, -1]\n",
    "        while (dataframe.shape[0] < max_mem_len):\n",
    "            dataframe = pd.concat([dataframe, dataframe])\n",
    "        return dataframe.values[:max_mem_len,:]\n",
    "        #os.remove(filepath)\n",
    "        #pass\n",
    "    elif (dataframe.shape[1] == 5 and dataframe.shape[0] <=max_mem_len):\n",
    "        counts[1] +=1\n",
    "        while (dataframe.shape[0] < max_mem_len):\n",
    "            dataframe = pd.concat([dataframe, dataframe])\n",
    "        return dataframe.values[:max_mem_len,:]\n",
    "    elif (dataframe.shape[1] == 5 and dataframe.shape[0] > max_mem_len):\n",
    "        counts[2] +=1\n",
    "        return dataframe.values[:max_mem_len,:]\n",
    "    else:\n",
    "        counts[3] +=1\n",
    "        print(\"1kernel\", filepath)\n",
    "        return \n",
    "\n",
    "def load_transfer_file(filepath):\n",
    "    if not os.path.exists(filepath):\n",
    "        print(\"transfer\", filepath)\n",
    "        return \n",
    "    dataframe = pd.read_csv(filepath)\n",
    "    if (dataframe.isnull().values.any()):\n",
    "        print(filepath)\n",
    "    if (dataframe.shape[1] == 4 and dataframe.shape[0] == 0):\n",
    "        print(\"0transfer\", filepath)\n",
    "        counts[0] +=1\n",
    "        return \n",
    "        #os.remove(filepath)\n",
    "        #pass\n",
    "    elif (dataframe.shape[1] == 4 and dataframe.shape[0] <=max_mem_len):\n",
    "        counts[1] +=1\n",
    "        while (dataframe.shape[0] < max_mem_len):\n",
    "            dataframe = pd.concat([dataframe, dataframe])\n",
    "        return dataframe.values[:max_mem_len,:]\n",
    "    elif (dataframe.shape[1] == 4 and dataframe.shape[0] > max_mem_len):\n",
    "        counts[2] +=1\n",
    "        return dataframe.values[:max_mem_len,:]\n",
    "    else:\n",
    "        counts[3] +=1\n",
    "        print(\"1transfer\", filepath)\n",
    "        return \n",
    "        \n",
    "def load_res_file(filepath, max_res_len):\n",
    "    dataframe = pd.read_csv(filepath)\n",
    "    if (dataframe.shape[1] == 4 and dataframe.shape[0] >= max_res_len):\n",
    "        #if dataframe[\"u_gpu\"].sum() == 0:\n",
    "            #os.remove(filepath)\n",
    "        #    print(\"none\", filepath)\n",
    "        #else:\n",
    "        return dataframe.values[:max_res_len,:]\n",
    "    else:\n",
    "        print(\"wrong\", filepath)\n",
    "\n",
    "def load_group(arch, sample_rate):\n",
    "\n",
    "    \n",
    "\n",
    "    y_label = []\n",
    "    \n",
    "    data_kernel_group = []\n",
    "    data_transfer_group = []\n",
    "    data_res_group = []\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    for category in [\"mybench\", \"risky\"]:\n",
    "        mem_pathfolder = '/home/pzou/projects/Power_Signature/results_backup/%s/%s/mem_trace-combine'%(category, arch)\n",
    "        res_pathfolder = '/home/pzou/projects/Power_Signature/results-%d/%s/%s/power-combine'%(sample_rate, category, arch)\n",
    "        app_list = get_app_list(\"/home/pzou/projects/Power_Signature/Scripts/applications-mem_%s.csv\"%(category))\n",
    "        for [app, num] in app_list:\n",
    "            \n",
    "            \n",
    "            if arch==\"k40\" and \"reductionMultiBlockCG\" in app:\n",
    "                continue\n",
    "            \n",
    "            kernel_fileName = app+num+\"_kernel.csv\"\n",
    "            data = load_kernel_file(os.path.join(mem_pathfolder, kernel_fileName))\n",
    "            data_kernel_group.append(data)\n",
    "    \n",
    "\n",
    "            transfer_fileName = app+num+\"_transfer.csv\"\n",
    "            data = load_transfer_file(os.path.join(mem_pathfolder, transfer_fileName))\n",
    "            data_transfer_group.append(data)\n",
    "            \n",
    "            \n",
    "            res_fileName =app+num+\"_int.csv\"\n",
    "            data = load_res_file(os.path.join(res_pathfolder, res_fileName), max_res_len)\n",
    "            data_res_group.append(data)\n",
    "\n",
    "            y_label.append(i)\n",
    "    \n",
    "        i += 1\n",
    "        \n",
    "    data_kernel_group = np.asarray(data_kernel_group)\n",
    "    data_transfer_group = np.asarray(data_transfer_group)\n",
    "    data_res_group = np.asarray(data_res_group)\n",
    "    \n",
    "    return data_kernel_group, data_transfer_group, data_res_group, y_label\n",
    "\n",
    "def rnnClassify(data_kernel_group, data_transfer_group, data_res_group, y_label,  arch, model_eval, ratio):\n",
    "    y_label = pd.Series(y_label)\n",
    "\n",
    "    fileM = \"Fusion\"\n",
    "    \n",
    "        \n",
    "    for idx in range(2):\n",
    "        test_index = [idx]\n",
    "        y_test = y_label[test_index]\n",
    "        X_kernel_test = data_kernel_group[test_index]\n",
    "        X_transfer_test = data_transfer_group[test_index]\n",
    "        X_res_test = data_res_group[test_index]\n",
    "\n",
    "        data_X = []\n",
    "        X_test = [X_kernel_test, X_transfer_test, X_res_test]\n",
    "\n",
    "        checkpoint_path = \"%s/%s-%s-%s-%d-W%d.hdf5\"%(arch,fileM ,arch, model_eval, 1, ratio )\n",
    "\n",
    "        model = tf.keras.models.load_model(checkpoint_path)\n",
    "        loss, accuracy = model.evaluate(x=X_test, y=y_test)\n",
    "\n",
    "\n",
    "        y_prob = model.predict([data_kernel_group, data_transfer_group, data_res_group])\n",
    "        y_predict = [int(i>=0.5) for i in y_prob]\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    print(\"start loading\")\n",
    "    max_mem_len = 64\n",
    "    sample_rate = 100\n",
    "    rate_len = {100:1800, 50:3600, 10:3600}\n",
    "    max_res_len = rate_len[sample_rate]\n",
    "    \n",
    "    arch=\"p100\"\n",
    "    data_kernel_group, data_transfer_group, data_res_group, y_label= load_group(arch, sample_rate)\n",
    "    print(\"Done loading\")\n",
    "    model_eval =  \"seen\"\n",
    "    ratio = 8\n",
    "    start = timeit.default_timer()\n",
    "    rnnClassify(data_kernel_group, data_transfer_group, data_res_group, y_label,  arch, model_eval, ratio)\n",
    "    stop = timeit.default_timer()\n",
    "\n",
    "    print('Time: ', stop - start)  \n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Anaconda 5.1.0)",
   "language": "python",
   "name": "anaconda3-5.1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
